# Logbook 9 â€“ Vergleich der Analysetools (Radon vs Gemini vs SonarQube)

## 1. Ziel
Ziel dieses Logbuchs ist es, die Ergebnisse der drei Analysetools **Radon**, **Gemini (LLM)** und **SonarQube** auf denselben Python-Code (`github_dijkstra.py`) systematisch zu vergleichen.  
Der Fokus liegt auf der Bewertung der **Analyse-Tiefe**, **ErkennungsqualitÃ¤t** und **praktischen NÃ¼tzlichkeit** der Metriken und Befunde.

---

## 2. Ãœberblick Ã¼ber den analysierten Code

Untersucht wurde der **Dijkstra-Algorithmus** in einer Python-Implementierung, die absichtlich verschiedene â€žCode Smellsâ€œ und ineffiziente Strukturen enthÃ¤lt, um das Verhalten der Tools besser zu evaluieren.  
Der Code umfasst ca. **100 Zeilen**, darunter eine Hauptfunktion (`dijkstra_naive`) mit einer **zyklomatischen KomplexitÃ¤t von 14â€“16**.

---

## 3. Ergebnisse im Ãœberblick

| Kategorie | Radon | Gemini (LLM) | SonarQube |
|------------|--------|--------------|------------|
| **Ziel der Analyse** | Statistische Code-Metriken | Semantische, kontextuelle Analyse | Regelbasierte QualitÃ¤tsprÃ¼fung |
| **CC (Zyklomatische KomplexitÃ¤t)** | 16 (C) | 14 | 14 |
| **Halstead Volume** | 237 | ~2299 | ~2200 (geschÃ¤tzt) |
| **Maintainability Index (MI)** | A (~90) | 37 | 37 |
| **Erkannte Code Smells** | Keine | 4 | 14 |
| **Cognitive Complexity** | â€” | Qualitativ beschrieben | 18 |
| **Duplications** | â€” | Erkannt (DRY-Verletzung) | ~15% |
| **Erkannte Bugs** | â€” | 1 potenziell | 3 potenziell |
| **Security Hotspots** | â€” | Keine | 1 (Performance Risk) |
| **VerbesserungsvorschlÃ¤ge** | Keine | Heap, Logging, DRY | Heap, Logging, TypprÃ¼fung, Docstrings |
| **Gesamtbewertung** | Gut strukturiert (formal) | Semantisch tiefgehend | Regelbasiert umfassend |

---

## 4. Interpretation der Ergebnisse

### ðŸ§© Radon
- Bewertet **nur syntaktische KomplexitÃ¤t** (CC, Halstead, MI).  
- Liefert stabile numerische Werte, aber **ignoriert logische und architektonische Probleme**.  
- Beurteilt die Datei fÃ¤lschlicherweise als *â€žhoch wartbarâ€œ*, da sie keine Ineffizienzen erkennt.

> ðŸ’¬ **Fazit:** Gut fÃ¼r Metrik-basierte Trends, aber nicht ausreichend fÃ¼r DesignqualitÃ¤t.

---

### ðŸ¤– Gemini (LLM)
- Analysiert den Code **kontextbezogen und semantisch**.  
- Erkennt ineffiziente Algorithmen (O(VÂ²)), Vermischung von Verantwortlichkeiten (print in Logik) und Redundanzen.  
- Liefert konkrete **Refactoring-VorschlÃ¤ge** und **architektonische Empfehlungen** (z. B. Einsatz von `heapq`, `TypeVar`, Logging).  
- Bewertet Wartbarkeit mit **MI â‰ˆ 37**, was realistisch ist.

> ðŸ’¬ **Fazit:** Liefert tiefgreifende qualitative Einsichten, Ã¼bertrifft klassische Metriken in VerstÃ¤ndnis und Kontext.

---

### ðŸ§± SonarQube
- FÃ¼hrt eine **regelbasierte statische Analyse** durch, Ã¤hnlich zu professionellen CI/CD-Pipelines.  
- Erkennt viele *Code Smells* (Duplikationen, ineffiziente Strukturen, fehlende Docstrings, Magic Numbers).  
- Bewertet Maintainability Index = 37 (Ã¼bereinstimmend mit Gemini).  
- ErgÃ¤nzt LLM-Ergebnisse durch strukturierte **Kategorisierung (Critical/Major/Minor)**.  
- Bietet quantifizierbare **Kennzahlen wie Cognitive Complexity und Duplications %.**

> ðŸ’¬ **Fazit:** Sehr solide, systematisch und reproduzierbar. Ideal zur Integration in professionelle Entwicklungsprozesse.

---

## 5. Wissenschaftliche Interpretation

Die Kombination der Tools zeigt, dass sich deren StÃ¤rken **gegenseitig ergÃ¤nzen**:

| Analysedimension | Radon | Gemini | SonarQube |
|------------------|--------|---------|------------|
| **Strukturelle Metriken** | âœ… | âš ï¸ (geschÃ¤tzt) | âœ… |
| **Semantisches VerstÃ¤ndnis** | âŒ | âœ…âœ…âœ… | âš ï¸ (Regelbasis) |
| **Architekturbewertung** | âŒ | âœ…âœ… | âœ… |
| **Wartbarkeitsprognose (MI)** | ÃœberschÃ¤tzt | Realistisch | Realistisch |
| **ErklÃ¤rungstiefe (Human Readability)** | Niedrig | Hoch | Mittel |
| **EmpfehlungsqualitÃ¤t** | Keine | Hoch (prÃ¤skriptiv) | Hoch (regelbasiert) |

> ðŸ“Š **Interpretation:**  
> - **Radon** = numerisch, schnell, aber oberflÃ¤chlich.  
> - **Gemini** = versteht den Code â€žwie ein Entwicklerâ€œ.  
> - **SonarQube** = erkennt systematische VerstÃ¶ÃŸe und ergÃ¤nzt die LLM-Sicht.  

Das Zusammenspiel dieser drei AnsÃ¤tze erlaubt eine **umfassende Bewertung von CodequalitÃ¤t, Wartbarkeit und ArchitekturverstÃ¤ndnis.**

---

## 6. Fazit

- **Radon** bietet eine solide quantitative Basis fÃ¼r die Metrikvergleiche.  
- **Gemini (LLM)** liefert eine qualitative Tiefe und erkennt semantische SchwÃ¤chen, die kein statisches Tool erfassen kann.  
- **SonarQube** verbindet beides durch regelbasierte Analyse und erzeugt ein industriell verwertbares QualitÃ¤tsprofil.  

> ðŸ” **Schlussfolgerung:**  
> Eine kombinierte Nutzung von **Radon + SonarQube + LLM** ermÃ¶glicht eine ganzheitliche Bewertung von SoftwarequalitÃ¤t:  
> - Radon â†’ objektive Metriken  
> - SonarQube â†’ regelbasierte Wartbarkeitsbewertung  
> - Gemini â†’ semantische und architektonische Intelligenz  

**Damit ist die empirische Analyse-Phase der Bachelorarbeit abgeschlossen.**

---

## 7. NÃ¤chste Schritte (optional)
- Integration der Ergebnisse in das Kapitel *â€žEvaluation der Toolsâ€œ*  
- Erstellung von Vergleichsgrafiken (Balkendiagramme der Metriken)  
- Vorbereitung des theoretischen Kapitels zur *â€žGrenzen statischer vs. LLM-basierter Analysenâ€œ*
